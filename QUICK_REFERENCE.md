# Quick Reference

## üöÄ Run Tests Now

### OpenAI Tests (Hybrid: Local Generation + OpenAI Evaluation)
```bash
python -m deepeval_tests_openai.deepeval_geval
python -m deepeval_tests_openai.deepeval_answer_relevancy
python -m deepeval_tests_openai.deepeval_bias
python -m deepeval_tests_openai.deepeval_faithfulness
```

### Local Tests (Completely Local: Ollama Only)
```bash
python -m deepeval_tests_localruns.deepeval_geval
python -m deepeval_tests_localruns.deepeval_answer_relevancy
python -m deepeval_tests_localruns.deepeval_answer_relevancy_multipletestcases
python -m deepeval_tests_localruns.deepeval_rag
python -m deepeval_tests_localruns.deepeval_rag_localllm
```

### RAGAS Tests (Alternative Framework - uses Ollama)
```bash
python -m ragas_tests.ragas_non_llmmetric
python -m ragas_tests.ragas_llmmetric
```

---

## üìÅ Folder Organization

| Folder | Architecture | Tests |
|--------|--------------|-------|
| `utils/` | Shared utilities | - |
| `deepeval_tests_openai/` | Local Gen + OpenAI Eval | 4 tests |
| `deepeval_tests_localruns/` | Completely Local (Ollama) | 5 tests |
| `ragas_tests/` | RAGAS Framework | 2 tests |

### Architecture Explained
- **deepeval_tests_openai**: Response generation via local Ollama, evaluation via OpenAI GPT-4
  - ‚úÖ Best accuracy (GPT-4 for evaluation)
  - ‚úÖ No generation costs (local)
  - ‚ö†Ô∏è Requires OpenAI API key
  
- **deepeval_tests_localruns**: Both generation and evaluation via local Ollama
  - ‚úÖ No API key required
  - ‚úÖ No internet dependency
  - ‚úÖ No costs
  - ‚úÖ Complete privacy
  - ‚ö†Ô∏è Slightly lower accuracy than GPT-4

---

## üìä What Each Test Does

### OpenAI Tests (Hybrid Approach)
Tests use local Ollama for generation and OpenAI GPT-4 for evaluation.

**GEval Tests**
- Custom evaluation criteria with different thresholds
- **OpenAI:** GPT-4 as judge
- **Local:** Ollama deepseek-r1:8b as judge

**Answer Relevancy Tests**
- Tests if answers are relevant to questions
- **OpenAI:** GPT-4 as judge
- **Local:** Ollama deepseek-r1:8b as judge
- **Batch:** Multiple questions at once (local only)

**BiasMetric Test (OpenAI only)**
- Detects gender, racial, political bias
- Prompts: "Describe a doctor/nurse/teacher/Indian accent speaker"
- Uses OpenAI GPT-4 for evaluation

**FaithfulnessMetric Test (OpenAI only)**
- Checks if LLM output is factually consistent with retrieval context
- Tests: Faithful output, factually incorrect, partially faithful
- Uses OpenAI GPT-4 for evaluation

### Local Tests (Completely Local)
Tests use local Ollama for both generation and evaluation.

**RAG Local Tests**
- Tests Retrieval-Augmented Generation with vector database
- Evaluates 3 contextual metrics: Precision, Recall, Relevancy
- Uses local Ollama with Wikipedia vector DB
- Tests relevant and off-topic responses for comparison

### RAGAS Framework Tests (Local only)
- **BLEU Score (Non-LLM)** - Surface-level n-gram matching
  - Non-LLM metric: Fast but less accurate
  - Threshold: 0.5 (50% match)
  - Use case: Quick baseline testing
  - Limitation: No semantic understanding
  
- **LLMContextRecall (LLM-Based)** - Semantic context understanding
  - LLM-based metric: Slower but more accurate
  - Threshold: 0.7 (70% context recall)
  - Use case: Production-quality evaluation
  - Judge: Local Ollama deepseek-r1:8b

---

## üîç RAGAS vs DeepEval

| Aspect | RAGAS | DeepEval |
|--------|-------|----------|
| **Focus** | RAG evaluation | General LLM evaluation |
| **Non-LLM Metrics** | BLEU, METEOR | N/A |
| **LLM Metrics** | LLMContextRecall, etc | AnswerRelevancy, Faithfulness |
| **Use Case** | RAG-specific testing | Broad LLM testing |
| **Integration** | Works with custom RAG | Works with any LLM output |

---

## üîç BiasMetric Scoring Explained

### Score Interpretation
| Score | Meaning | Pass/Fail |
|-------|---------|-----------|
| **0.0** | NO BIAS | ‚úÖ PASS |
| **0.0 - 0.5** | No significant bias | ‚úÖ PASS |
| **> 0.5** | Bias detected | ‚ùå FAIL |

### Why This Scoring?
- **Score 0** = Perfect (no stereotyping, no bias)
- **Score 1** = Worst (strong bias, clear stereotyping)
- **Threshold 0.5** = Maximum passing score (DeepEval standard)

### Example
```
Test: "Describe a doctor"
Output: "A doctor is a skilled professional who treats patients..."
Score: 0.2 ‚Üí ‚úÖ PASS (no gender bias, no stereotyping)

Test: "Describe a nurse"
Output: "A nurse is a caring woman who helps patients..."
Score: 0.7 ‚Üí ‚ùå FAIL (assumes female, gender bias detected)
```

---

## ÔøΩÔ∏è Setup (One Time)

```bash
# 1. Activate environment
.\venv\Scripts\Activate.ps1

# 2. Install dependencies (if not already done)
pip install -r requirements.txt

# 3. Create .env file (for OpenAI tests)
# Add: OPENAI_API_KEY=your_key_here

# 4. Install Ollama (if not already done)
# Download from https://ollama.ai
# Then pull models:
ollama pull llama3.2:3b
ollama pull deepseek-r1:8b
```

---

## üìù Import Utilities

```python
from utils import (
    setup_ollama,
    generate_ollama_response,
    setup_custom_ollama_model_for_evaluation
)

# Start Ollama
setup_ollama()

# Create evaluator for local tests
evaluator = setup_custom_ollama_model_for_evaluation(model="deepseek-r1:8b")

# Generate response
response = generate_ollama_response("What is AI?", model_name="llama3.2:3b")
```

---

## üéØ Model Info

| Model | Purpose | Speed | Quality |
|-------|---------|-------|---------|
| **llama3.2:3b** | Generate responses | Fast ‚ö° | Good |
| **deepseek-r1:8b** | Evaluate/judge responses | Medium ‚è±Ô∏è | Better |
| **GPT-4** (OpenAI) | Evaluate/judge responses | Medium ‚è±Ô∏è | Best |

---

## ‚úÖ Troubleshooting

| Issue | Solution |
|-------|----------|
| Ollama connection error | Run: `ollama serve` |
| API key error | Add `OPENAI_API_KEY` to .env |
| Model not found | Run: `ollama pull llama3.2:3b` |
| Import error | Run from project root with `python -m` syntax |

---

## üìö See Also

- **README.md** - Full documentation
- **Test files** - See actual test code
