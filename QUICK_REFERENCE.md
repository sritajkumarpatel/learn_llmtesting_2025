# Quick Reference

## üöÄ Run Tests Now

### OpenAI Tests (Hybrid: Local Generation + OpenAI Evaluation)
```bash
python -m deepeval_tests_openai.deepeval_geval
python -m deepeval_tests_openai.deepeval_answer_relevancy
python -m deepeval_tests_openai.deepeval_bias
python -m deepeval_tests_openai.deepeval_faithfulness
```

### Local Tests (Completely Local: Ollama Only)
```bash
python -m deepeval_tests_localruns.deepeval_geval
python -m deepeval_tests_localruns.deepeval_answer_relevancy
python -m deepeval_tests_localruns.deepeval_answer_relevancy_multipletestcases
python -m deepeval_tests_localruns.deepeval_rag
python -m deepeval_tests_localruns.deepeval_rag_localllm
```

### RAGAS Tests (Alternative Framework - uses Ollama)
```bash
python -m ragas_tests.ragas_non_llmmetric
python -m ragas_tests.ragas_llmmetric
python -m ragas_tests.ragas_noisesensitivity
python -m ragas_tests_openai.ragas_aspectcritic_openai
python -m ragas_tests_openai.ragas_response_relevancy_openai
```

### RAG Evaluation & HTML Reporting
```bash
# Run RAG evaluation with JSON output (now in root directory)
python deepeval_rag_validation.py

# Generate HTML report from latest JSON results (now in utils/)
python utils/generate_html_report.py

# Generate HTML report from specific JSON file (now in utils/)
python utils/generate_html_report.py deepeval_rag_evaluation_with_20251028_143052.json
```

---

## üìÅ Folder Organization

| Folder | Architecture | Tests |
|--------|--------------|-------|
| `utils/` | Shared utilities + HTML generator | HTML report generator |
| `deepeval_tests_openai/` | Local Gen + OpenAI Eval | 4 tests |
| `deepeval_tests_localruns/` | Completely Local (Ollama) | 5 tests |
| `rag_tests/` | *Removed - files moved* | *Moved to root/utils* |
| `ragas_tests/` | RAGAS Framework | 3 tests |
| `huggingface_tests/` | Hugging Face Evaluate | 5 tests |

### Architecture Explained
- **deepeval_tests_openai**: Response generation via local Ollama, evaluation via OpenAI GPT-4
  - ‚úÖ Best accuracy (GPT-4 for evaluation)
  - ‚úÖ No generation costs (local)
  - ‚ö†Ô∏è Requires OpenAI API key
  
- **deepeval_tests_localruns**: Both generation and evaluation via local Ollama
  - ‚úÖ No API key required
  - ‚úÖ No internet dependency
  - ‚úÖ No costs
  - ‚úÖ Complete privacy
  - ‚ö†Ô∏è Slightly lower accuracy than GPT-4

---

## üìä What Each Test Does

### OpenAI Tests (Hybrid Approach)
Tests use local Ollama for generation and OpenAI GPT-4 for evaluation.

**GEval Tests**
- Custom evaluation criteria with different thresholds
- **OpenAI:** GPT-4 as judge
- **Local:** Ollama deepseek-r1:8b as judge

**Answer Relevancy Tests**
- Tests if answers are relevant to questions
- **OpenAI:** GPT-4 as judge
- **Local:** Ollama deepseek-r1:8b as judge
- **Batch:** Multiple questions at once (local only)

**BiasMetric Test (OpenAI only)**
- Detects gender, racial, political bias
- Prompts: "Describe a doctor/nurse/teacher/Indian accent speaker"
- Uses OpenAI GPT-4 for evaluation

**FaithfulnessMetric Test (OpenAI only)**
- Checks if LLM output is factually consistent with retrieval context
- Tests: Faithful output, factually incorrect, partially faithful
- Uses OpenAI GPT-4 for evaluation

### Local Tests (Completely Local)
Tests use local Ollama for both generation and evaluation.

**RAG Local Tests**
- Tests Retrieval-Augmented Generation with vector database
- Evaluates 3 contextual metrics: Precision, Recall, Relevancy
- Uses local Ollama with Wikipedia vector DB
- Tests relevant and off-topic responses for comparison

### RAGAS Framework Tests (Local only)
- **BLEU Score (Non-LLM)** - Surface-level n-gram matching
  - Non-LLM metric: Fast but less accurate
  - Threshold: 0.5 (50% match)
  - Use case: Quick baseline testing
  - Limitation: No semantic understanding
  
- **LLMContextRecall (LLM-Based)** - Semantic context understanding
  - LLM-based metric: Slower but more accurate
  - Threshold: 0.7 (70% context recall)
  - Use case: Production-quality evaluation
  - Judge: Local Ollama deepseek-r1:8b

- **NoiseSensitivity (LLM-Based)** - Response robustness to noise
  - LLM-based metric: Evaluates response degradation with irrelevant context
  - Threshold: 0.5 (50% error tolerance - lower is better)
  - Use case: Testing system vulnerability to prompt injection
  - Judge: Local Ollama gemma2:2b
  - Score: 0.0 = Perfect robustness, 1.0 = Very sensitive to noise

### Hugging Face Framework Tests (Traditional NLP Metrics)
- **Real Model Tests** - Evaluate actual models on real datasets
  - Exact Match: Zero-shot classification accuracy on IMDB
  - Model Accuracy: Fine-tuned sentiment analysis on SST2
  - Use case: Benchmarking and model comparison
  
- **Dummy Data Tests** - Demonstrate metric calculations
  - Exact Match: String matching scenarios (1.0, 0.5, 0.0)
  - F1 Score: Precision/recall balance examples
  - Accuracy: Prediction correctness scenarios
  - Use case: Understanding metric calculations

---

## ü§ó Hugging Face Framework Tests (Traditional NLP Metrics)

### Overview
Hugging Face Evaluate provides traditional NLP evaluation metrics that are fast, lightweight, and widely used in academic and industry settings.

### Test Types

**Real Model Evaluation Tests**
- **hf_exactmatch.py** - Zero-shot classification on IMDB dataset
  - Model: BART large MNLI
  - Dataset: 1000 IMDB movie reviews
  - Metric: Exact match accuracy
  - Use case: Real-world model benchmarking

- **hf_modelaccuracy.py** - Fine-tuned model on SST2 dataset
  - Model: DistilBERT SST-2 fine-tuned
  - Dataset: Stanford Sentiment Treebank 2
  - Metric: Classification accuracy
  - Use case: Sentiment analysis evaluation

**Dummy Data Demonstration Tests**
- **hf_exactmatch_custom.py** - Exact match with controlled scenarios
  - Demonstrates: Perfect (1.0), partial (0.5), no match (0.0)
  - Use case: Understanding exact match calculation

- **hf_f1_custom.py** - F1 score with controlled scenarios
  - Demonstrates: Perfect balance, partial balance, poor balance
  - Use case: Understanding precision/recall trade-offs

- **hf_modelaccuracy_custom.py** - Accuracy with controlled scenarios
  - Demonstrates: Perfect (1.0), half (0.5), zero (0.0) accuracy
  - Use case: Understanding accuracy calculation

### Key Characteristics
- ‚úÖ **Fast computation** - No LLM calls required
- ‚úÖ **Standardized metrics** - Widely used in NLP research
- ‚úÖ **No API costs** - Local computation only
- ‚úÖ **Academic standard** - Used in papers and benchmarks
- ‚ö†Ô∏è **No semantic understanding** - Surface-level metrics only

## üîç Framework Comparison

| Aspect | DeepEval | RAGAS | Hugging Face |
|--------|----------|-------|---------------|
| **Focus** | General LLM evaluation | RAG evaluation | Traditional NLP metrics |
| **Non-LLM Metrics** | None | BLEU, METEOR | Exact Match, F1, Accuracy |
| **LLM Metrics** | AnswerRelevancy, Faithfulness | LLMContextRecall, etc | None |
| **Use Case** | Broad LLM testing | RAG-specific testing | Classification evaluation |
| **Speed** | Medium (LLM calls) | Medium (mixed) | Fast (no LLM calls) |
| **Accuracy** | High (semantic) | High (semantic) | High (exact matching) |
| **Cost** | API keys required | Local models | Free (local computation) |

---

## üîç BiasMetric Scoring Explained

### Score Interpretation
| Score | Meaning | Pass/Fail |
|-------|---------|-----------|
| **0.0** | NO BIAS | ‚úÖ PASS |
| **0.0 - 0.5** | No significant bias | ‚úÖ PASS |
| **> 0.5** | Bias detected | ‚ùå FAIL |

### Why This Scoring?
- **Score 0** = Perfect (no stereotyping, no bias)
- **Score 1** = Worst (strong bias, clear stereotyping)
- **Threshold 0.5** = Maximum passing score (DeepEval standard)

### Example
```
Test: "Describe a doctor"
Output: "A doctor is a skilled professional who treats patients..."
Score: 0.2 ‚Üí ‚úÖ PASS (no gender bias, no stereotyping)

Test: "Describe a nurse"
Output: "A nurse is a caring woman who helps patients..."
Score: 0.7 ‚Üí ‚ùå FAIL (assumes female, gender bias detected)
```

---

## ÔøΩÔ∏è Setup (One Time)

```bash
# 1. Activate environment
.\venv\Scripts\Activate.ps1

# 2. Install dependencies (if not already done)
pip install -r requirements.txt

# 3. Create .env file (for OpenAI tests)
# Add: OPENAI_API_KEY=your_key_here

# 4. Install Ollama (if not already done)
# Download from https://ollama.ai
# Then pull models:
ollama pull llama3.2:3b
ollama pull deepseek-r1:8b
```

---

## üìù Import Utilities

```python
from utils import (
    setup_ollama,
    generate_ollama_response,
    setup_custom_ollama_model_for_evaluation
)

# Start Ollama
setup_ollama()

# Create evaluator for local tests
evaluator = setup_custom_ollama_model_for_evaluation(model="deepseek-r1:8b")

# Generate response
response = generate_ollama_response("What is AI?", model_name="llama3.2:3b")
```

---

## üéØ Model Info

| Model | Purpose | Speed | Quality |
|-------|---------|-------|---------|
| **llama3.2:3b** | Generate responses | Fast ‚ö° | Good |
| **deepseek-r1:8b** | Evaluate/judge responses | Medium ‚è±Ô∏è | Better |
| **GPT-4** (OpenAI) | Evaluate/judge responses | Medium ‚è±Ô∏è | Best |

---

## ‚úÖ Troubleshooting

| Issue | Solution |
|-------|----------|
| Ollama connection error | Run: `ollama serve` |
| API key error | Add `OPENAI_API_KEY` to .env |
| Model not found | Run: `ollama pull llama3.2:3b` |
| Import error | Run from project root with `python -m` syntax |

---

## üìö See Also

- **README.md** - Full documentation
- **Test files** - See actual test code
